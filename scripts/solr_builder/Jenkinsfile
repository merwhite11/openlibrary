pipeline {
  agent {
    dockerfile {
      filename 'Dockerfile'
      dir 'scripts/solr_builder'
      // Needed to make docker-in-docker work
      args '-v /var/run/docker.sock:/var/run/docker.sock'
      args '-v /var/lib/docker/volumes/jenkins-data:/var/lib/docker/volumes/jenkins-data'
      // Where to store dumps
      args '-v /storage:/storage'
    }
  }
  parameters {
    string(name: 'PARTIAL_IMPORT_SIZE', defaultValue: '', description: 'If set to a number, will only import that many records from the dump (useful for testing!)')
    booleanParam(name: 'REUSE_POSTGRES', defaultValue: false, description: 'If true, assumes a functioning postgres is running/correct')
  }
  environment {
    // The *host* location of Jenkins directory
    JENKINS_HOME_DIR = sh(script: "docker volume inspect --format='{{.Mountpoint}}' jenkins-data", returnStdout: true).trim()
    // Where to save dumps
    DUMP_DIR = '/storage/openlibrary'
  }
  stages {
    stage('1: Create a local postgres copy of the database') {
      when { expression { return !params.REUSE_POSTGRES } }
      environment {
        // Where to download the ol full dump from
        OL_DUMP_LINK = 'https://openlibrary.org/data/ol_dump_latest.txt.gz'
        // Get the date-suffixed name of the latest dump
        FULL_OL_DUMP_FILE = sh(script: "curl '${env.OL_DUMP_LINK}' -s -L -I -o /dev/null -w '%{url_effective}'", returnStdout: true).trim().split('/').last()
        // Nmae of partial dump file
        PARTIAL_OL_DUMP_FILE = "partial_${env.FULL_OL_DUMP_FILE}"
        // Use partial dump file, if specified so in the parameters
        OL_DUMP_FILE = "${params.PARTIAL_IMPORT_SIZE ? env.PARTIAL_OL_DUMP_FILE : env.FULL_OL_DUMP_FILE}"
      }
      stages {
        stage('Setup') {
          parallel {
            stage('Launch postgres containers') {
              steps {
                dir(env.HOST_SOLR_BUILDER_DIR) {
                  sh(label: 'Launch the database container',
                    script: 'docker-compose up -d --no-deps db')

                  sh(label: 'optional; GUI for the database at port 8087',
                    script: 'docker-compose up -d --no-deps adminer')

                  // Wait for postgres
                  sleep 10

                  sh(label: 'Create the table to store the dump',
                    script: 'docker-compose exec -T -u postgres db psql -d postgres -f sql/create-dump-table.sql')
                }
              }
            }
            stage('Setup dump file') {
              stages {
                stage('Download dump file') {
                  steps {
                    dir(env.DUMP_DIR) {
                      sh "wget --progress=dot:giga --trust-server-names --no-clobber ${env.OL_DUMP_LINK}"
                    }
                  }
                }
                stage('Create partial dump file') {
                  when { expression { return params.PARTIAL_IMPORT_SIZE } }
                  steps {
                    dir(env.DUMP_DIR) {
                      sh "zcat ${env.FULL_OL_DUMP_FILE} | head -n ${params.PARTIAL_IMPORT_SIZE} | gzip > ${env.PARTIAL_OL_DUMP_FILE}"
                    }
                  }
                }
              }
            }
          }
        }
        stage('Populate postgres') {
          stages {
            stage('Import dump into postgres') {
              environment {
                PARALLEL_PROCESSES = '6'
              }
              steps {
                dir(env.HOST_SOLR_BUILDER_DIR) {
                  sh(label: 'Import dump into postgres',
                    script: "./psql-import-in-chunks.sh ${env.DUMP_DIR}/${env.OL_DUMP_FILE} ${env.PARALLEL_PROCESSES}")
                  
                  waitUntil {
                    script {
                      // Once completed, the log file says "COPY 8828224" (or however many records)
                      // error if we have anything else
                      // (cat at the end to avoid grep exiting when no match)
                      def error_output = sh(script: 'cat logs/psql-chunk-* | grep -v COPY | cat', returnStdout: true)
                      assert !error_output
                      
                      def cores_completed = sh(script: 'cat logs/psql-chunk-* | grep COPY | wc -l', returnStdout: true).trim()
                      echo "Completed ${cores_completed} core(s) of ${env.PARALLEL_PROCESSES}"
                      return cores_completed == env.PARALLEL_PROCESSES
                    }
                  }
                }
              }
              post {
                always {
                  dir(env.HOST_SOLR_BUILDER_DIR) {
                    sh 'for f in logs/psql-chunk-*.txt; do echo "$f: "; cat "$f"; done > logs/psql-import.log'
                    archiveArtifacts artifacts: 'logs/psql-import.log'
                  }
                }
              }
            }
            stage('Test import successful') {
              steps {
                dir(env.HOST_SOLR_BUILDER_DIR) {
                  script {
                    def lines_in_dump = sh(script: "cd ${env.DUMP_DIR}; zcat ${env.OL_DUMP_FILE} | wc -l", returnStdout: true).trim()
                    echo lines_in_dump
                    def rows_in_db = sh(script: '''
                      . ./aliases.sh
                      psql -q -f sql/count-all-rows.sql''', returnStdout: true).trim()
                    echo rows_in_db
                    assert lines_in_dump == rows_in_db
                  }
                }
              }
            }
            stage('Create postgres indices') {
              steps {
                dir(env.HOST_SOLR_BUILDER_DIR) {
                  sh "docker-compose exec -T -u postgres db psql postgres -f sql/create-indices.sql | ts '[%Y-%m-%d %H:%M:%S]'"
                }
              }
            }
          }
        }
      }
      post {
        always {
          dir(env.DUMP_DIR) {
            sh(label: 'Remove partial dump file (if we had one)',
              script: "rm -f ${env.PARTIAL_OL_DUMP_FILE}")
          }
        }

        unsuccessful {
          dir(env.HOST_SOLR_BUILDER_DIR) {
            sh 'docker-compose stop'
            sh 'docker-compose rm -f'
            sh 'docker volume rm solr_builder_postgres-data'
          }
        }
      }
    }
    stage('2: Populate solr') {
      environment {
        // Where the solr dumps are saved
        SOLR_DUMP_DIR = "${env.DUMP_DIR}/solr"
        // The name of expanded solr dump
        CUR_SOLR_BACKUP = '_solr-backup-cur'
        // The full path of the solr backup
        SOLR_BACKUP_PATH = "${env.SOLR_DUMP_DIR}/${env.CUR_SOLR_BACKUP}"
        // The location of the repo on the host machine; needed for docker-in-docker images
        HOST_SOLR_BUILDER_DIR = "${env.WORKSPACE}/scripts/solr_builder".replaceFirst('/var/jenkins_home', env.JENKINS_HOME_DIR)
        // As specified in docker-compose.yml
        NEW_SOLR_URL = 'http://localhost:8984'
        BACKUP_SOLR_URL = 'http://localhost:8985'
      }
      stages {
        stage('Setup') {
          steps {
            dir(env.HOST_SOLR_BUILDER_DIR) {
              sh(label: 'Build the solr image',
                script: 'docker build -t olsolr:latest -f ../../docker/Dockerfile.olsolr ../../')
              
              sh(label: 'Launch solr',
                script: 'docker-compose up --no-deps -d solr')

              sh(label: 'Build "lite" ol environment',
                script: 'docker-compose build ol')
              
              sh(label: 'Build the cython files',
                script: 'docker-compose run ol ./build-cython.sh')
              
              sh(label: 'Load a helper function into postgres',
                script: '''. ./aliases.sh
                          psql -f sql/get-partition-markers.sql''')
            }
          }
        }
        stage('Insert works & orphaned editions') {
          steps {
            dir(env.HOST_SOLR_BUILDER_DIR) {
              sh './index-works.sh'
              sh './index-orphans.sh'

              waitUntil {
                script {
                  print_indexing_progress('{works_work,orphans}')
                  return count_indexers_completed('{works_work,orphans}') == '6'
                }
              }

              solr_commit(env.NEW_SOLR_URL)
            }
          }
          post {
            always {
              dir(env.HOST_SOLR_BUILDER_DIR) {
                sh 'for f in logs/works_work*; do echo "$f: "; cat "$f"; done > logs/works-indexing.log'
                sh 'for f in logs/works_orphans*; do echo "$f: "; cat "$f"; done > logs/orphans-indexing.log'
                archiveArtifacts artifacts: 'logs/{works,orphans}-indexing.log'
              }
            }
          }
        }
        stage('Insert authors') {
          steps {
            dir(env.HOST_SOLR_BUILDER_DIR) {
              sh './index-authors.sh'

              waitUntil {
                script {
                  print_indexing_progress('work_authors')
                  return count_indexers_completed('work_authors') == '6'
                }
              }

              solr_commit(env.NEW_SOLR_URL)
            }
          }
          post {
            always {
              dir(env.HOST_SOLR_BUILDER_DIR) {
                sh 'for f in logs/work_authors*; do echo "$f: "; cat "$f"; done > logs/authors-indexing.log'
                archiveArtifacts artifacts: 'logs/authors-indexing.log'
              }
            }
          }
        }
        stage('Insert subjects') {
          input {
            id "Download dump to ${env.SOLR_DUMP_DIR}"
            message 'Is the file in the right location yet?'
            ok 'Yes'
            parameters {
              string(name: 'SOLR_BACKUP_FILE', defaultValue: '', description: 'What is it called?')
            }
          }
          stages {
            stage('Decompress the dump') {
              steps {
                dir(env.SOLR_DUMP_DIR) {
                  sh "mkdir ${env.CUR_SOLR_BACKUP}"
                  sh "tar xzf ${SOLR_BACKUP_FILE} -C ${env.CUR_SOLR_BACKUP}"
                }
              }
            }
            stage('Launch solr-backup service') {
              steps {
                dir(env.HOST_SOLR_BUILDER_DIR) {
                  sh "docker-compose up -d --no-deps solr-backup"
                  script {
                    // Assert backup has subjects in it
                    assert solr_query_count(env.BACKUP_SOLR_URL, 'subject') != '0'
                  }
                }
              }
            }
            stage('Copy subjects into main solr') {
              steps {
                sh "curl -s ${env.NEW_SOLR_URL}/solr/dataimport?command=full-import"

                waitUntil {
                  script {
                    def status_resp_cmd = "curl -s ${env.NEW_SOLR_URL}:8985/solr/dataimport"
                    def status_cmd = """${status_resp_cmd} | xmlstarlet sel -t -v "//str[@name='status']" """
                    def status = sh(script: status_cmd, returnStdout: true).trim()
                    return status == '?????'
                  }
                }

                script {
                  def backup_subjects_count = solr_query_count(env.BACKUP_SOLR_URL, 'subject')
                  def new_subjects_count = solr_query_count(env.NEW_SOLR_URL, 'subject')
                  assert backup_subjects_count == new_subjects_count
                }
              }
            }
          }
        }
      }
    }
  }
  post {
    always {
      dir(env.SOLR_DUMP_DIR) {
        sh(label: 'Remove solr backup directory',
          script: "rm -rf ${env.CUR_SOLR_BACKUP}")
      }

      deleteDir() // Delete the workspace
    }
  }
}

String solr_query_count(String solr_base_url, String type_to_count) {
  def solr_url = "${solr_url}/solr/select/?q=type%3A${type_to_count}&rows=0"
  def count_cmd = "curl -s '${solr_url}' | xmlstarlet sel -t -v '//result/@numFound'"
  def count = sh(script: count_subjects_cmd, returnStdout: true).trim()
  return count
}

void solr_commit(String solr_base_url) {
  sh(label: 'Solr commit the changes',
    script: "curl ${solr_base_url}/solr/update?commit=true")
}

void print_indexing_progress(String prefix_glob) {
  sh(label: 'Progress',
    script: """
    for f in progress/${prefix_glob}*; do 
      printf "\$f: ";
      tail -n1 \$f | grep -oe '[0-9]*\\.[0-9]*%';
    done; """)
}

String count_indexers_completed(String prefix_glob) {
  return sh(script: """
    for f in progress/${prefix_glob}*; do
      tail -n1 \$f; 
    done | grep 100.00% | wc -l
    """,
    returnStdout: true).trim()
}
